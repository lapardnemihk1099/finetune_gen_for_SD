{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "snZ-BIvlNgX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAJpcShOTgVL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers sentencepiece boto3 sacrebleu datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install nltk -U --quiet"
      ],
      "metadata": {
        "id": "saL5fmp_Xz3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install jsonlines"
      ],
      "metadata": {
        "id": "xK7Gg6LA18_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "import os\n",
        "import boto3\n",
        "import torch\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, MarianMTModel, MarianTokenizer\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "from tqdm.notebook import tqdm\n",
        "import logging\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "import jsonlines\n",
        "import json\n",
        "from jsonlines import jsonlines\n",
        "import numpy as np\n",
        "\n",
        "tqdm.pandas()\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "T793qcTjVix7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "gLpVlPLKUk7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cnc_path = '/content/drive/MyDrive/CausalNewsCorpus/data' \n",
        "# def process_cnc_data(data_type):\n",
        "#   df = pd.read_csv(os.path.join(cnc_path, f\"{data_type}.csv\"))\n",
        "#   df.drop(['corpus', 'doc_id', 'sent_id', 'eg_id', 'index', 'seq_label', 'pair_label', 'context', 'num_sents'], axis=1, inplace=True)\n",
        "#   df.rename(columns={\n",
        "#       'text' : 'input_text',\n",
        "#       'text_w_pairs': 'target_text'\n",
        "#   }, inplace=True)\n",
        "#   df.to_csv(f'/content/{data_type}.csv', index=False)\n",
        "# process_cnc_data('train_subtask2')\n",
        "# process_cnc_data('dev_subtask2')"
      ],
      "metadata": {
        "id": "KYXErY-vGiuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_checkpoints = \"facebook/mbart-large-50\"\n",
        "# train_model_checkpoints = \"t5-large\""
      ],
      "metadata": {
        "id": "N6asO19hkYmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MarianMT tokenizer\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(train_model_checkpoints, src_lang=\"en_XX\", tgt_lang=\"en_XX\")\n",
        "# tokenizer = T5Tokenizer.from_pretrained(train_model_checkpoints)"
      ],
      "metadata": {
        "id": "3Y4nxVACkcUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def add_prompt(source_list, target_list):\n",
        "#   processed_input = []\n",
        "#   for input, target in zip(source_list, target_list):\n",
        "#     ts_ratio = len(target)/len(input)\n",
        "#     if ts_ratio < 0.95:\n",
        "#       prefix = \"paraphrase short\"\n",
        "#     elif ts_ratio >= 0.95 and ts_ratio <= 1.10:\n",
        "#       prefix = \"paraphrase normal\"\n",
        "#     else:\n",
        "#       prefix = \"paraphrase long\"\n",
        "#     input = prefix + \" \" + input\n",
        "#     processed_input.append(input)\n",
        "#   return processed_input"
      ],
      "metadata": {
        "id": "2X22_OcYeHA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# session = boto3.Session(\n",
        "#     aws_access_key_id='**********************',\n",
        "#     aws_secret_access_key='****************************',\n",
        "# )\n",
        "# s3 = session.resource('s3')\n",
        "# key = \"unicausal_data\"\n",
        "# filename = \"./unicausal.csv\"\n",
        "# print(key)\n",
        "# s3.meta.client.download_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "pJhFoZlxFlx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = boto3.Session(\n",
        "    aws_access_key_id='***********************',\n",
        "    aws_secret_access_key='**********************',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = \"CASE2022-train-subtask2\"\n",
        "filename = \"./train_subtask2.csv\"\n",
        "print(key)\n",
        "s3.meta.client.download_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "EiCcfKhgmf_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess MUST-C dataset\n",
        "max_input_length = 128 \n",
        "max_target_length = 128\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"input_text\"]\n",
        "    targets = examples[\"target_text\"]\n",
        "    inputs = [f\"CASE: {text}\" for text in inputs]\n",
        "   # inputs = add_prompt(inputs, targets) # append appropriate prompts \n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "    # setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "-yIynk8nkQTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_directory = os.getcwd()\n",
        "path_to_train_data = os.path.join(current_directory, \"./train_subtask2.csv\")\n",
        "# path_to_validation_data = os.path.join(current_directory, \"dev_subtask2.csv\")\n",
        "raw_train_dataset = load_dataset('csv', data_files={\"train\": path_to_train_data})\n",
        "# raw_validation_dataset = load_dataset('csv', data_files={\"validation\": path_to_validation_data})"
      ],
      "metadata": {
        "id": "uQM_O1wLgBTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_dataset"
      ],
      "metadata": {
        "id": "C3aNpskjw_PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize raw data\n",
        "tokenized_train_datasets = raw_train_dataset[\"train\"].map(preprocess_function, batched=True)\n",
        "# tokenized_validation_datasets = raw_validation_dataset[\"validation\"].map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "m7Pdj6mZpgXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# session = boto3.Session(\n",
        "#     aws_access_key_id='************************',\n",
        "#     aws_secret_access_key='*********************',\n",
        "# )\n",
        "# s3 = session.resource('s3')\n",
        "# key = \"CASE22_fewshot-learning-mbart-large-50-finetuned-for-span-detection\"\n",
        "# filename = \"fewshot-learning-mbart-large-50-finetuned-for-span-detection.zip\"\n",
        "# print(key)\n",
        "# s3.meta.client.download_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "WsG4LEYVkhF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ./fewshot-learning-mbart-large-50-finetuned-for-span-detection.zip -d ./fewshot-learning-mbart-large-50-finetuned-for-span-detection-cluster"
      ],
      "metadata": {
        "id": "e1XWRBQTko06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training procedure\n",
        "model_checkpoints = \"./fewshot-learning-mbart-large-50-finetuned-for-span-detection-cluster/checkpoint-7300/\"\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_checkpoints)\n",
        "# model = T5ForConditionalGeneration.from_pretrained(train_model_checkpoints, return_dict=True)"
      ],
      "metadata": {
        "id": "8allrAVnghNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4 # change batch-size according to GPU availability \n",
        "model_name = train_model_checkpoints.split(\"/\")[-1]\n",
        "epoch = 20\n",
        "strategy = \"epoch\"\n",
        "\n",
        "# define training model arguments\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"fewshot-learning-{model_name}-finetuned-for-span-detection\",\n",
        "    learning_rate=5e-5, \n",
        "    logging_strategy=strategy,\n",
        "    # learning_rate=0.0003,\n",
        "    # lr_scheduler_type=\"linear\",\n",
        "    # warmup_ratio=0.06,\n",
        "    # optim=\"adafactor\",\n",
        "    save_strategy=strategy,\n",
        "    # save_steps=save_steps_,\n",
        "    # evaluation_strategy=strategy,\n",
        "    # eval_steps=steps_,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    # per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epoch,\n",
        "    # report_to=\"wandb\",\n",
        "    save_total_limit=1,\n",
        "    predict_with_generate=True    \n",
        ")\n",
        "\n",
        "# initialize data-collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "U6cLMMcWhNJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sacrebleu = load_metric(\"sacrebleu\")\n",
        "meteor = load_metric(\"meteor\")\n",
        "f1_metric = load_metric(\"f1\")\n",
        "\n",
        "# import numpy as np\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "    \n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    f1_result = f1_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    sacrebleu_result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    meteor_result = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\n",
        "        \"bleu\": sacrebleu_result[\"score\"],\n",
        "        \"meteor\": meteor_result['meteor'],\n",
        "        \"f1\": f1_result[\"f1\"],\n",
        "    }\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    print(result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "jPOvjOI5jYJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the trainer module\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    # eval_dataset=tokenized_validation_datasets,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "EBWGqtxirB7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Usr-bx_vgZzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "devices = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "0Fz1XfytpWzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_checkpoints = \"./drive/MyDrive/CASE22/fewshot-learning-mbart-large-50-finetuned-for-span-detection/checkpoint-460\"\n",
        "model_checkpoints = \"./fewshot-learning-mbart-large-50-finetuned-for-span-detection/checkpoint-5834\"\n",
        "# model_checkpoints = \"./drive/MyDrive/CASE22/fewshot-learning-t5-large-finetuned-for-span-detection/checkpoint-920\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(train_model_checkpoints, src_lang=\"en_XX\", tgt_lang=\"en_XX\")\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_checkpoints).to(devices)\n",
        "# tokenizer = T5Tokenizer.from_pretrai  ned(model_checkpoints)\n",
        "# model = T5ForConditionalGeneration.from_pretrained(model_checkpoints, return_dict=True).to(devices)"
      ],
      "metadata": {
        "id": "ttd79E1_6Gli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_list = []\n",
        "df = pd.read_csv('./dev_subtask2.csv')\n",
        "for _, row in tqdm(df.iterrows(),total=df.shape[0]):\n",
        "  decoded_output = model.generate(\n",
        "      **tokenizer(f\"CASE: {row['input_text']}\", return_tensors='pt').to(devices),\n",
        "      max_length=256\n",
        "  )\n",
        "  output = [tokenizer.decode(e,skip_special_tokens=True) for e in decoded_output][0]\n",
        "  pred_list.append(output)\n",
        "\n",
        "df['pred'] = pred_list\n",
        "df.to_csv(f\"./proc_dev_subtask2_{model_name}.csv\", index=False)\n",
        "\n",
        "# !mv ./fewshot-learning-t5-large-finetuned-for-span-detection/ ./drive/MyDrive/CASE22/fewshot-learning-t5-large-finetuned-for-span-detection/\n",
        "# !mv ./fewshot-learning-mbart-large-50-finetuned-for-span-detection/ ./drive/MyDrive/CASE22/fewshot-learning-mbart-large-50-finetuned-for-span-detection"
      ],
      "metadata": {
        "id": "l1mV2boS88Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "tOEJhvYCvl7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[16]['pred'], df.iloc[17]['pred']"
      ],
      "metadata": {
        "id": "wBA_SnrpxDP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = df.iloc[16]\n",
        "print(sample['input_text'])\n",
        "\n",
        "# decoded_output = model.generate(\n",
        "#     **tokenizer(sample['input_text'], return_tensors='pt').to(devices)\n",
        "# )\n",
        "decoded_output = model.generate(\n",
        "    **tokenizer(f\"CASE: {sample['input_text']}\", return_tensors='pt').to(devices), \n",
        "    # do_sample=True,\n",
        "    # top_p=0.95, \n",
        "    # top_k=50,\n",
        "    num_return_sequences=5\n",
        ")\n",
        "# decoded_output = model.generate(\n",
        "#     **tokenizer(sample['input_text'], return_tensors='pt').to(devices), \n",
        "#     num_beams=10,\n",
        "#     early_stopping=True, \n",
        "#     num_return_sequences=4\n",
        "# )"
      ],
      "metadata": {
        "id": "QlqJMOx3ZKa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = [tokenizer.decode(e,skip_special_tokens=True) for e in decoded_output]"
      ],
      "metadata": {
        "id": "ePdy3GPjw05b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "iRRn_tUdw2H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# current_directory = os.getcwd()\n",
        "# model_checkpoints = f\"fewshot-learning-{model_name}-finetuned-for-span-detection\"\n",
        "# model_checkpoint_directory = os.path.join(current_directory, model_checkpoints)\n",
        "# print(model_checkpoint_directory)   \n",
        "# shutil.make_archive(model_checkpoint_directory, \"zip\", model_checkpoint_directory.split('/')[-1])"
      ],
      "metadata": {
        "id": "NCdziurcHLpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# session = boto3.Session(\n",
        "#     aws_access_key_id='******************',\n",
        "#     aws_secret_access_key='*************************',\n",
        "# )\n",
        "# s3 = session.resource('s3')\n",
        "# key = f\"COLAB_CASE22_{model_checkpoints}\"\n",
        "# filename = os.path.join(current_directory, f\"{model_checkpoints}.zip\")\n",
        "# print(key)\n",
        "# s3.meta.client.upload_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "ecUpYkB1Hkod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_list = []\n",
        "df = pd.read_csv('./dev_subtask2_text.csv')\n",
        "# df.drop_duplicates(subset=['input_text'], inplace=True)\n",
        "# df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "output_list = [] \n",
        "\n",
        "for index, row in tqdm(df.iterrows(),total=df.shape[0]):\n",
        "  current_dict = dict()\n",
        "  current_dict['index'] = index\n",
        "  decoded_output = model.generate(\n",
        "      **tokenizer(f\"CASE: {row['text']}\", return_tensors='pt').to(devices),\n",
        "      num_return_sequences=3\n",
        "  )\n",
        "  output = [tokenizer.decode(e,skip_special_tokens=True) for e in decoded_output]\n",
        "  current_dict['prediction'] = output\n",
        "  # current_dict['target_text'] = row['target_text']\n",
        "  output_list.append(current_dict)"
      ],
      "metadata": {
        "id": "fV9y-r0JzXJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open('devsubtask2_submission_DF.json','w') as writer:\n",
        "  writer.write_all(output_list)"
      ],
      "metadata": {
        "id": "6VQ0r0vj1fz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_list = []\n",
        "df = pd.read_csv('./test_subtask2_text.csv')\n",
        "# df.drop_duplicates(subset=['input_text'], inplace=True)\n",
        "# df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "output_list = [] \n",
        "\n",
        "for index, row in tqdm(df.iterrows(),total=df.shape[0]):\n",
        "  current_dict = dict()\n",
        "  current_dict['index'] = index\n",
        "  decoded_output = model.generate(\n",
        "      **tokenizer(f\"CASE: {row['text']}\", return_tensors='pt').to(devices),\n",
        "      num_return_sequences=3\n",
        "  )\n",
        "  output = [tokenizer.decode(e,skip_special_tokens=True) for e in decoded_output]\n",
        "  current_dict['prediction'] = output\n",
        "  # current_dict['target_text'] = row['target_text']\n",
        "  output_list.append(current_dict)"
      ],
      "metadata": {
        "id": "FQVeAgLbANLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open('testsubtask2_submission_DF.json','w') as writer:\n",
        "  writer.write_all(output_list)"
      ],
      "metadata": {
        "id": "FnFIBUmmlPr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1TCv7UjWlSPx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}